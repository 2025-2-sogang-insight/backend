import json
import os
import shutil
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from tqdm import tqdm
from .settings import JSON_DIR, DB_PATH, EMBEDDING_MODEL

def clean_source_name(filename_stem):
    """
    íŒŒì¼ëª… ì •ì œ: "preprocessed_ì„¸íŠ¸(ë¦¬ê·¸-ì˜¤ë¸Œ-ë ˆì „ë“œ)" -> "ì„¸íŠ¸"
    """
    name = filename_stem.replace("preprocessed_", "")
    name = name.replace("(ë¦¬ê·¸-ì˜¤ë¸Œ-ë ˆì „ë“œ)", "")
    return name.strip()

def create_vector_db():
    # 1. JSON í´ë” í™•ì¸
    if not os.path.exists(JSON_DIR):
        print(f"âŒ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {JSON_DIR}")
        return

    documents = []
    files = list(JSON_DIR.glob("*.json"))
    
    if not files:
        print(f"âŒ '{JSON_DIR}' ì•ˆì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê¸°ì¡´ DB ì‚­ì œ (ëª¨ë¸ ë³€ê²½ ì‹œ í•„ìˆ˜)
    if os.path.exists(DB_PATH):
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ DB í´ë”ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤: {DB_PATH}")
        shutil.rmtree(DB_PATH)
    
    print(f"ğŸ“‚ ì´ {len(files)}ê°œì˜ JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

    # 3. íŒŒì¼ ë¡œë“œ ë° ë¬¸ì„œ ìƒì„±
    for file_path in tqdm(files):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                clean_name = clean_source_name(file_path.stem)
                
                if "sections" in data:
                    for section in data["sections"]:
                        heading = section.get("heading", "")
                        text = section.get("text", "")
                        
                        if not text.strip(): continue
                        
                        # ë‚´ìš© êµ¬ì„±: [ì±”í”¼ì–¸ëª…] ì†Œì œëª© + ë‚´ìš©
                        content = f"[{clean_name}] {heading}\n{text}"
                        metadata = {
                            "source": clean_name,
                            "heading": heading,
                            "filename": file_path.name
                        }
                        documents.append(Document(page_content=content, metadata=metadata))
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")

    if not documents:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    # 5. ì„ë² ë”© ë° DB ì €ì¥ (OpenAI)
    print(f"â³ OpenAI ì„ë² ë”©({EMBEDDING_MODEL})ìœ¼ë¡œ DB êµ¬ì¶• ì¤‘...")
    
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    
    Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=str(DB_PATH)
    )
    
    print("-" * 50)
    print(f"ğŸ‰ DB êµ¬ì¶• ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {DB_PATH}")
    print("-" * 50)

if __name__ == "__main__":
    create_vector_db()