import json
import os
import shutil
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from tqdm import tqdm

# [ìˆ˜ì •] JSON_DIR_OPGG ì¶”ê°€ ì„í¬íŠ¸
from .settings import JSON_DIR, JSON_DIR_OPGG, DB_PATH, EMBEDDING_MODEL

def clean_source_name(filename_stem):
    """
    íŒŒì¼ëª… ì •ì œ: "preprocessed_ì„¸íŠ¸(ë¦¬ê·¸-ì˜¤ë¸Œ-ë ˆì „ë“œ)" -> "ì„¸íŠ¸"
    """
    name = filename_stem.replace("preprocessed_", "")
    name = name.replace("(ë¦¬ê·¸-ì˜¤ë¸Œ-ë ˆì „ë“œ)", "")
    return name.strip()

def create_vector_db():
    # 1. ì²˜ë¦¬í•  ë°ì´í„° í´ë” ëª©ë¡ ì •ì˜
    # ë‚˜ë¬´ìœ„í‚¤ ë°ì´í„° ê²½ë¡œì™€ OP.GG ë°ì´í„° ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
    source_dirs = [
        {"path": JSON_DIR, "category": "namuwiki"},
        {"path": JSON_DIR_OPGG, "category": "opgg"}
    ]
    
    all_files = []
    
    print("ğŸ“‚ ë°ì´í„° í´ë”ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...")
    for source in source_dirs:
        dir_path = source["path"]
        category = source["category"]
        
        if os.path.exists(dir_path):
            files = list(dir_path.glob("*.json"))
            print(f"   - [{category}] {len(files)}ê°œì˜ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ({dir_path})")
            # íŒŒì¼ ê²½ë¡œì™€ ì¹´í…Œê³ ë¦¬ë¥¼ í•¨ê»˜ ì €ì¥
            for f in files:
                all_files.append({"file_path": f, "category": category})
        else:
            print(f"   âš ï¸ [{category}] í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {dir_path}")

    if not all_files:
        print("âŒ ì²˜ë¦¬í•  JSON íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê¸°ì¡´ DB ì‚­ì œ (ëª¨ë¸ ë³€ê²½/ë°ì´í„° ê°±ì‹  ì‹œ í•„ìˆ˜)
    if os.path.exists(DB_PATH):
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ DB í´ë”ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤: {DB_PATH}")
        shutil.rmtree(DB_PATH)
    
    documents = []
    print(f"ğŸš€ ì´ {len(all_files)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

    # 3. íŒŒì¼ ë¡œë“œ ë° ë¬¸ì„œ ìƒì„±
    for item in tqdm(all_files):
        file_path = item["file_path"]
        category = item["category"]
        
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                clean_name = clean_source_name(file_path.stem)
                
                # ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬
                # Case A: "sections" í‚¤ê°€ ìˆëŠ” ê²½ìš° (ë‚˜ë¬´ìœ„í‚¤ êµ¬ì¡° ë“±)
                if "sections" in data:
                    for section in data["sections"]:
                        heading = section.get("heading", "")
                        text = section.get("text", "")
                        
                        if not text.strip(): continue
                        
                        # ë‚´ìš© êµ¬ì„±: [ì¹´í…Œê³ ë¦¬:ì±”í”¼ì–¸ëª…] ì†Œì œëª© + ë‚´ìš©
                        content = f"[{category.upper()} | {clean_name}] {heading}\n{text}"
                        metadata = {
                            "source": clean_name,
                            "category": category, # namuwiki ë˜ëŠ” opgg
                            "heading": heading,
                            "filename": file_path.name
                        }
                        documents.append(Document(page_content=content, metadata=metadata))
                
                # Case B: "sections"ê°€ ì—†ê³  ë°”ë¡œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° (OP.GG ë‹¨ìˆœ ë°ì´í„° ë“±)
                # ë§Œì•½ OP.GG ë°ì´í„° êµ¬ì¡°ê°€ ë‹¤ë¥´ë‹¤ë©´ ì´ ë¶€ë¶„ì„ ì»¤ìŠ¤í…€í•´ì•¼ í•©ë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²½ìš° ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ë´…ë‹ˆë‹¤.
                else:
                    text_content = json.dumps(data, ensure_ascii=False, indent=2)
                    content = f"[{category.upper()} | {clean_name}] ì „ì²´ ë°ì´í„°\n{text_content}"
                    metadata = {
                        "source": clean_name,
                        "category": category,
                        "heading": "Full Data",
                        "filename": file_path.name
                    }
                    documents.append(Document(page_content=content, metadata=metadata))

        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")

    if not documents:
        print("âŒ ìƒì„±ëœ ë¬¸ì„œ(Documents)ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    # 5. ì„ë² ë”© ë° DB ì €ì¥ (OpenAI)
    print(f"â³ OpenAI ì„ë² ë”©({EMBEDDING_MODEL})ìœ¼ë¡œ DB êµ¬ì¶• ì¤‘...")
    
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    
    Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=str(DB_PATH)
    )
    
    print("-" * 50)
    print(f"ğŸ‰ DB êµ¬ì¶• ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {DB_PATH}")
    print(f"   - ì´ ì²˜ë¦¬ íŒŒì¼: {len(all_files)}ê°œ")
    print(f"   - ì´ ì²­í¬ ìˆ˜: {len(splits)}ê°œ")
    print("-" * 50)

if __name__ == "__main__":
    create_vector_db()